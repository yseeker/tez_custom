{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "seti_my_customNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1urRIQRoh9-gWVmm9KTWjExME5kVcjBWv",
      "authorship_tag": "ABX9TyPRRVJxVCXEMaYl0epS5IMp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8d97cf14aba9409a817d480f1011743d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_73784d45422b44f5b529ec525a6bb37c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e2f091c8ef3b45b6b9b401b86bc6d0db",
              "IPY_MODEL_8aaeac2bc9244651a0b58e6b15e2496d"
            ]
          }
        },
        "73784d45422b44f5b529ec525a6bb37c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e2f091c8ef3b45b6b9b401b86bc6d0db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_59982be56d15496c987ef1d6ce6b66f9",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 2508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_512b8abc8c7d4ed496eba9bccee3426a"
          }
        },
        "8aaeac2bc9244651a0b58e6b15e2496d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cc06a5d335bf43028258837bf6305750",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/2508 [00:57&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc38748ce4da4cf2bb771eaaa51a86a9"
          }
        },
        "59982be56d15496c987ef1d6ce6b66f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "512b8abc8c7d4ed496eba9bccee3426a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cc06a5d335bf43028258837bf6305750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc38748ce4da4cf2bb771eaaa51a86a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yseeker/tez_custom/blob/main/docs/seti_my_customNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xDJzJCP7G-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5913c9b1-c653-48c0-a165-0a19de81fa3d"
      },
      "source": [
        "!nvidia-smi\n",
        "!pip install --upgrade albumentations\n",
        "class CFG:\n",
        "    project_name = 'SETI_aug_check2'\n",
        "    pretrained_model_name = 'efficientnetv2_rw_s'\n",
        "    pretrained = True\n",
        "    prettained_path = '../input/timm_weight/efficientnet_b0_ra-3dd342df.pth'\n",
        "    mixup_alpha = 0.5\n",
        "    lr = 1.0e-6\n",
        "    batch_size= 16\n",
        "    wandb_note = f'tawara_bs{batch_size}_adamW_default_lr{lr}_alpha{mixup_alpha}'\n",
        "    input_channels = 1\n",
        "    out_dim = 1\n",
        "    colab_or_kaggle = 'colab'\n",
        "    wandb_exp_name = f'{pretrained_model_name}_{colab_or_kaggle}_{wandb_note}'\n",
        "    epochs = 60\n",
        "    num_of_fold = 5\n",
        "    seed = 42\n",
        "    patience = 3\n",
        "    delta = 0.002\n",
        "    num_workers = 8\n",
        "    fp16 = True\n",
        "    checkpoint_path = ''\n",
        "    patience_mode = 'max'\n",
        "    patience = 3\n",
        "    delta = 0.002\n",
        "    dims_head = [None, 1]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jun 26 16:02:00 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Collecting albumentations\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/be/3db3cd8af771988748f69eace42047d5edebf01eaa7e1293f3b3f75f989e/albumentations-1.0.0-py3-none-any.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Collecting opencv-python-headless>=4.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/35/bfc76533f2274cd3da4e2cf255cd13ab9d7f6fc8990c06911e7f8fcc2130/opencv_python_headless-4.5.2.54-cp37-cp37m-manylinux2014_x86_64.whl (38.2MB)\n",
            "\u001b[K     |████████████████████████████████| 38.2MB 148kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.5.1)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
            "Installing collected packages: opencv-python-headless, albumentations\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.0.0 opencv-python-headless-4.5.2.54\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAD1kL_xEzUj"
      },
      "source": [
        "import os\n",
        "import json\n",
        "f = open(\"/content/drive/My Drive/kaggle/kaggle.json\", 'r')\n",
        "json_data = json.load(f) #JSON形式で読み込む\n",
        "os.environ['KAGGLE_USERNAME'] = json_data['username']\n",
        "os.environ['KAGGLE_KEY'] = json_data['key']\n",
        "os.chdir(\"/content/drive/My Drive/kaggle/working\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGbEvBi_GweX",
        "outputId": "a681910e-675c-48c7-b94b-b385fc678340"
      },
      "source": [
        "os.cpu_count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTPtXCdJE3wK",
        "outputId": "e837a0cc-2024-4c6a-d26f-3f34719fc451"
      },
      "source": [
        "!pip install wandb\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "#from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "#from tqdm import tqdm_notebook as tqdm\n",
        "import datetime\n",
        "import psutil\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchsummary import summary\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "\n",
        "import wandb\n",
        "import warnings\n",
        "\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/b4/9d92953d8cddc8450c859be12e3dbdd4c7754fb8def94c28b3b351c6ee4e/wandb-0.10.32-py2.py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 8.1MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/91/b38c4fabb6e5092ab23492ded4f318ab7299b19263272b703478038c0fbc/GitPython-3.1.18-py3-none-any.whl (170kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 58.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 14.0MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 58.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (57.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6502 sha256=072f003e8abb3d4d183c77fec72b791f679cb8af546d78259264dffb70365a0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8807 sha256=a8ab6c20deaa221281482d72f992a8402e5872c6df9451d8c9f99f2bc885a760\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, gitdb, GitPython, subprocess32, pathtools, shortuuid, sentry-sdk, docker-pycreds, configparser, wandb\n",
            "Successfully installed GitPython-3.1.18 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.1.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w1ohCx7FD2R"
      },
      "source": [
        "#!pip install timm\n",
        "tez_path = '../input/tez-lib'\n",
        "efnet_path = '../input/efficientnet-pytorch/EfficientNet-PyTorch/EfficientNet-PyTorch-master'\n",
        "timm_path = '../input/pytorch-image-models-master'\n",
        "sys.path.append(tez_path)\n",
        "sys.path.append(efnet_path)\n",
        "sys.path.append(timm_path)\n",
        "\n",
        "import timm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZv1EYLzpGF2"
      },
      "source": [
        "timm_efficientnet_family = set(['mnasnet_050', 'mnasnet_075', 'mnasnet_100', 'mnasnet_140', 'semnasnet_050', 'semnasnet_075', 'semnasnet_100', 'semnasnet_140', 'mnasnet_small', 'mobilenetv2_100', 'mobilenetv2_110d', 'mobilenetv2_120d', 'mobilenetv2_140', 'fbnetc_100', 'spnasnet_100', 'eca_efficientnet_b0', 'gc_efficientnet_b0', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'efficientnet_b8', 'efficientnet_l2', 'efficientnet_es', 'efficientnet_em', 'efficientnet_el', 'efficientnet_es_pruned', 'efficientnet_el_pruned', 'efficientnet_cc_b0_4e', 'efficientnet_cc_b0_8e', 'efficientnet_cc_b1_8e', 'efficientnet_lite0', 'efficientnet_lite1', 'efficientnet_lite2', 'efficientnet_lite3', 'efficientnet_lite4', 'efficientnet_b1_pruned', 'efficientnet_b2_pruned', 'efficientnet_b3_pruned', 'efficientnetv2_rw_s', 'efficientnetv2_rw_m', 'efficientnetv2_s', 'efficientnetv2_m', 'efficientnetv2_l', 'mobilenetv3_rw'])\n",
        "timm_resnet_family = set(['resnet18', 'resnet18d', 'resnet34', 'resnet34d', 'resnet26', 'resnet26d', 'resnet26t', 'resnet50', 'resnet50d', 'resnet50t', 'resnet101', 'resnet101d', 'resnet152', 'resnet152d', 'resnet200', 'resnet200d', 'tv_resnet34', 'tv_resnet50', 'tv_resnet101', 'tv_resnet152', 'wide_resnet50_2', 'wide_resnet101_2', 'resnext50_32x4d', 'resnext50d_32x4d', 'resnext101_32x4d', 'resnext101_32x8d', 'resnext101_64x4d', 'tv_resnext50_32x4d', 'ig_resnext101_32x8d', 'ig_resnext101_32x16d', 'ig_resnext101_32x32d', 'ig_resnext101_32x48d', 'ssl_resnet18', 'ssl_resnet50', 'ssl_resnext50_32x4d', 'ssl_resnext101_32x4d', 'ssl_resnext101_32x8d', 'ssl_resnext101_32x16d', 'swsl_resnet18', 'swsl_resnet50', 'swsl_resnext50_32x4d', 'swsl_resnext101_32x4d', 'swsl_resnext101_32x8d', 'swsl_resnext101_32x16d', 'seresnet18', 'seresnet34', 'seresnet50', 'seresnet50t', 'seresnet101', 'seresnet152', 'seresnet152d', 'seresnet200d', 'seresnet269d', 'seresnext26d_32x4d', 'seresnext26t_32x4d', 'seresnext50_32x4d', 'seresnext101_32x4d', 'seresnext101_32x8d', 'senet154', 'ecaresnet26t', 'ecaresnetlight', 'ecaresnet50d', 'ecaresnet50d_pruned', 'ecaresnet50t', 'ecaresnet101d', 'ecaresnet101d_pruned', 'ecaresnet200d', 'ecaresnet269d', 'ecaresnext26t_32x4d', 'ecaresnext50t_32x4d', 'resnetblur18', 'resnetblur50', 'resnetrs50', 'resnetrs101', 'resnetrs152', 'resnetrs200', 'resnetrs270', 'resnetrs350', 'resnetrs420'])\n",
        "timm_vit_family = set(['vit_small_patch16_224', 'vit_base_patch16_224', 'vit_base_patch32_224', 'vit_base_patch16_384', 'vit_base_patch32_384', 'vit_large_patch16_224', 'vit_large_patch32_224', 'vit_large_patch16_384', 'vit_large_patch32_384', 'vit_base_patch16_224_in21k', 'vit_base_patch32_224_in21k', 'vit_large_patch16_224_in21k', 'vit_large_patch32_224_in21k', 'vit_huge_patch14_224_in21k', 'vit_deit_tiny_patch16_224', 'vit_deit_small_patch16_224', 'vit_deit_base_patch16_224', 'vit_deit_base_patch16_384', 'vit_deit_tiny_distilled_patch16_224', 'vit_deit_small_distilled_patch16_224', 'vit_deit_base_distilled_patch16_224', 'vit_deit_base_distilled_patch16_384', 'vit_base_patch16_224_miil_in21k', 'vit_base_patch16_224_miil'])\n",
        "timm_swin_transformer = set(['swin_base_patch4_window12_384', 'swin_base_patch4_window7_224', 'swin_large_patch4_window12_384', 'swin_large_patch4_window7_224', 'swin_small_patch4_window7_224', 'swin_tiny_patch4_window7_224', 'swin_base_patch4_window12_384_in22k', 'swin_base_patch4_window7_224_in22k', 'swin_large_patch4_window12_384_in22k', 'swin_large_patch4_window7_224_in22k'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6ULGIlopdyk"
      },
      "source": [
        "class ClassificationDataset():\n",
        "    def __init__(self, image_paths, targets, transform = None): \n",
        "        self.image_paths = image_paths\n",
        "        self.targets = targets\n",
        "        self.transform = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, item): \n",
        "        targets = self.targets[item]\n",
        "        #image1 = np.load(self.image_paths[item]).astype(float)\n",
        "        #image1 = np.load(self.image_paths[item])[::2].astype(np.float32)\n",
        "        image1 = np.load(self.image_paths[item])[[0,2,4,1,3,5]]\n",
        "        image = np.vstack(image1).transpose((1, 0))\n",
        "\n",
        "        # image = ((image - np.mean(image, axis=1, keepdims=True)) / np.std(image, axis=1, keepdims=True))\n",
        "        # image = ((image - np.mean(image, axis=0, keepdims=True)) / np.std(image, axis=0, keepdims=True))\n",
        "    \n",
        "        #image = image.astype(np.float32)[np.newaxis, ]\n",
        "        image = image.astype(\"f\")[..., np.newaxis]  # shape: (256, 819, 1)\n",
        "\n",
        "        # image = np.load(self.image_paths[item]).astype(np.float32)\n",
        "        # image = np.vstack(image).transpose((1, 0))\n",
        "        # image = cv2.resize(image, dsize=(224,224), interpolation=cv2.INTER_CUBIC)\n",
        "        # image = image[np.newaxis, :, :]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)[\"image\"]\n",
        "\n",
        "        return torch.tensor(image, dtype=torch.float), torch.tensor(targets, dtype=torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i88MLFLA0gvh"
      },
      "source": [
        "def set_seed(seed = 0):\n",
        "    np.random.seed(seed)\n",
        "    random_state = np.random.RandomState(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    return random_state\n",
        "\n",
        "class AverageMeter:\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIgBPBdWGYLR"
      },
      "source": [
        "\n",
        "\n",
        "train_aug = A.Compose(\n",
        "    [\n",
        "        A.Resize(p = 1.0, height = 320, width = 320),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.ShiftScaleRotate(p=0.5,\n",
        "                           shift_limit = 0.2, \n",
        "                           scale_limit=0.2,\n",
        "                           rotate_limit=30, \n",
        "                           border_mode = cv2.BORDER_REPLICATE),\n",
        "        A.RandomResizedCrop(\n",
        "            p = 1.0,\n",
        "            height = 320,\n",
        "            width = 320,\n",
        "            scale = [0.9, 1.0]\n",
        "        ),\n",
        "        A.OneOf([\n",
        "            A.MedianBlur(p=0.3),\n",
        "            A.MotionBlur(p=0.3)\n",
        "        ]\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "df = pd.read_csv('../input/seti-breakthrough-listen/train_labels.csv')\n",
        "df['img_path'] = df['id'].apply(\n",
        "    lambda x: f'../input/seti-breakthrough-listen/train/{x[0]}/{x}.npy'\n",
        ")\n",
        "X = df.img_path.values\n",
        "Y = df.target.values\n",
        "skf = StratifiedKFold(n_splits = CFG.num_of_fold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLK7xeqRu9v3"
      },
      "source": [
        "def mixup_data(inputs, targets, alpha=1.0):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = inputs.size()[0]\n",
        "    index = torch.randperm(batch_size)\n",
        "    mixed_inputs = lam * inputs + (1 - lam) * inputs[index, :]\n",
        "    targets_a, targets_b = targets, targets[index]\n",
        "    \n",
        "    return mixed_inputs, targets_a, targets_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, outputs, targets_a, targets_b, lam):\n",
        "    return lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DYCYENwjwvj"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(CFG.pretrained_model_name, \n",
        "                                       pretrained = CFG.pretrained, \n",
        "                                       in_chans = CFG.input_channels)\n",
        "        if not CFG.pretrained: self.model.load_state_dict(torch.load(CFG.pretrained_path))\n",
        "        self.model.classifier = nn.Linear(self.model.classifier.in_features, CFG.out_dim)\n",
        "        #self.fc = ppe.nn.LazyLinear(None, CFG.out_dim)\n",
        "        self.conv1 = nn.Conv2d(1, 3, \n",
        "                               kernel_size=3, \n",
        "                               stride=1, \n",
        "                               padding=3, \n",
        "                               bias=False)\n",
        "        \n",
        "        dims_head = CFG.dims_head\n",
        "        if dims_head[0] is None:\n",
        "            dims_head[0] = self.model.classifier.in_features\n",
        "        layers_list = []\n",
        "        for i in range(len(dims_head) - 2):\n",
        "            in_dim, out_dim = dims_head[i: i + 2]\n",
        "            layers_list.extend([\n",
        "                nn.Linear(in_dim, out_dim),\n",
        "                nn.ReLU(), nn.Dropout(0.5),])\n",
        "        layers_list.append(\n",
        "            nn.Linear(dims_head[-2], dims_head[-1]))\n",
        "        self.head_cls = nn.Sequential(*layers_list)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        #x = self.conv1(inputs)\n",
        "        x = self.model(inputs)\n",
        "        outputs = self.head_cls(x)\n",
        "        return outputs\n",
        "\n",
        "class Trainer():\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        train_dataset,\n",
        "        valid_dataset,\n",
        "        train_batchsize,\n",
        "        valid_batchsize,\n",
        "        valid_targets,\n",
        "        num_workers,\n",
        "        fp16,\n",
        "        multiple_GPU,\n",
        "        determinstic,\n",
        "        benchmark,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.valid_targets = valid_targets\n",
        "        self.fp16 = fp16\n",
        "        self.scaler = torch.cuda.amp.GradScaler()\n",
        "        self.optimizer = self.configure_optimizer()\n",
        "        self.scheduler_after_step = self.configure_scheduler_after_step()\n",
        "        self.scheduler_after_epoch = self.configure_scheduler_after_epoch()\n",
        "        torch.backends.cudnn.deterministic = determinstic\n",
        "        torch.backends.cudnn.benchmark = benchmark \n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        if num_workers == -1: num_workers = psutil.cpu_count()\n",
        "        self.multiple_GPU = multiple_GPU\n",
        "        if multiple_GPU and torch.cuda.device_count() > 1:\n",
        "            print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "            self = nn.DataParallel(self)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        self.train_loader = torch.utils.data.DataLoader(\n",
        "            dataset = train_dataset, \n",
        "            batch_size = train_batchsize,\n",
        "            shuffle=True, \n",
        "            num_workers= num_workers,\n",
        "            drop_last = True,\n",
        "            pin_memory = True\n",
        "        )\n",
        "        self.valid_loader = torch.utils.data.DataLoader(\n",
        "            dataset = valid_dataset, \n",
        "            batch_size=valid_batchsize,\n",
        "            shuffle=False, \n",
        "            num_workers = num_workers,\n",
        "            drop_last = False,\n",
        "            pin_memory = True\n",
        "        )\n",
        "\n",
        "    def _init_wandb(self, cfg):\n",
        "        hyperparams = {\n",
        "            'batch_size' : cfg.batch_size,\n",
        "            'epochs' : cfg.epochs\n",
        "        }\n",
        "        wandb.init(\n",
        "            config = hyperparams,\n",
        "            project= cfg.project_name,\n",
        "            name=cfg.wandb_exp_name,\n",
        "        )\n",
        "        wandb.watch(self.model)\n",
        "\n",
        "    def configure_optimizer(self):\n",
        "        opt = torch.optim.Adam(self.parameters(), lr=CFG.lr)\n",
        "        #opt = torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9)\n",
        "        return opt\n",
        "    \n",
        "    def configure_scheduler_after_step(self):\n",
        "        sch = torch.optim.lr_scheduler.OneCycleLR(\n",
        "            optimizer = self.optimizer,\n",
        "            epochs = CFG.epochs,\n",
        "            steps_per_epoch = 2508,\n",
        "            max_lr = 5.0e-4,\n",
        "            pct_start = 0.1,\n",
        "            anneal_strategy = 'cos',\n",
        "            div_factor = 1.0e+3,\n",
        "            final_div_factor = 1.0e+3\n",
        "        )\n",
        "        # sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        #     self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n",
        "        # )\n",
        "        #sch = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=7, gamma=0.1)\n",
        "        #sch = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=5e-4, gamma=0.9, cycle_momentum=False,\n",
        "        #step_size_up=1400,step_size_down=1400, mode=\"triangular2\")\n",
        "        return sch\n",
        "\n",
        "    def configure_scheduler_after_epoch(self):\n",
        "        return None\n",
        "\n",
        "    def epoch_metrics(self, outputs, targets):\n",
        "        return metrics.roc_auc_score(targets, outputs)\n",
        "\n",
        "    def monitor_metrics(self, outputs, targets):\n",
        "        outputs = outputs.cpu().detach().numpy()\n",
        "        targets = targets.cpu().detach().numpy()\n",
        "        if len(np.unique(targets)) > 1: \n",
        "            roc_auc = metrics.roc_auc_score(targets, outputs)\n",
        "        else: roc_auc = 1.0\n",
        "        return roc_auc\n",
        "\n",
        "    def train_one_step(self, inputs, targets):\n",
        "        inputs = inputs.to(self.device, non_blocking=True)\n",
        "        targets = targets.to(self.device, non_blocking=True)\n",
        "        inputs, targets_a, targets_b, lam = mixup_data(inputs, \n",
        "                                                      targets,\n",
        "                                                      alpha= CFG.mixup_alpha)\n",
        "        self.optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(True):\n",
        "            if self.fp16:\n",
        "                with torch.cuda.amp.autocast(self.fp16):\n",
        "                    outputs = self.model(inputs)\n",
        "                    #loss = self.criterion(outputs, targets.view(-1, 1))\n",
        "                    loss = mixup_criterion(self.criterion,\n",
        "                                                outputs, \n",
        "                                                targets_a.view(-1, 1),\n",
        "                                                targets_b.view(-1, 1), \n",
        "                                                lam)\n",
        "                    metrics = self.monitor_metrics(outputs, targets)\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "            else:\n",
        "                outputs = self(inputs)\n",
        "                metrics = self.monitor_metrics(outputs, targets)\n",
        "                loss = self.criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "            if self.scheduler_after_step:\n",
        "                self.scheduler_after_step.step()\n",
        "        return outputs, loss, metrics\n",
        "\n",
        "    def validate_one_step(self, inputs, targets = None):\n",
        "        inputs = inputs.to(self.device, non_blocking=True)\n",
        "        if targets is not None:\n",
        "            targets = targets.to(self.device, non_blocking=True)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, targets.view(-1, 1))\n",
        "                metrics = self.monitor_metrics(outputs, targets)\n",
        "            return outputs, loss, metrics\n",
        "        else:\n",
        "            outputs = self(inputs)\n",
        "            return outputs, None, None\n",
        "\n",
        "    def predict_one_step(self, inputs):\n",
        "        outputs, _, _ = self.validate_one_step(inputs)\n",
        "        return outputs\n",
        "        \n",
        "    def train_one_epoch(self, data_loader):\n",
        "        self.model.train()\n",
        "        running_loss, running_metrics = AverageMeter(), AverageMeter()\n",
        "        tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True)\n",
        "        for batch_idx, (inputs, targets) in enumerate(tk0):\n",
        "            preds_one_batch, loss, metrics = self.train_one_step(inputs, targets)\n",
        "            running_loss.update(loss.item(), data_loader.batch_size)\n",
        "            running_metrics.update(metrics, data_loader.batch_size)\n",
        "            current_lr = self.optimizer.param_groups[0]['lr'] \n",
        "            wandb.log({\n",
        "                \"train/step\" : b_idx,\n",
        "                \"train/loss_step\": losses.avg,\n",
        "                \"lr\": current_lr \n",
        "                }\n",
        "            tk0.set_postfix(train_loss=running_loss.avg, train_step_metrics = running_metrics.avg, stage=\"train\", lr = current_lr)\n",
        "        if self.scheduler_after_epoch:\n",
        "            self.scheduler_after_epoch.step()\n",
        "        tk0.close()\n",
        "        return running_loss.avg\n",
        "\n",
        "    def validate_one_epoch(self, data_loader):\n",
        "        self.model.eval()\n",
        "        running_loss, running_metrics = AverageMeter(), AverageMeter()\n",
        "        preds_list = []\n",
        "        tk0 = tqdm(data_loader, total=len(data_loader), position = 0, leave = True)\n",
        "        for batch_idx, (inputs, targets) in enumerate(tk0):\n",
        "            preds_one_batch, loss, metrics = self.validate_one_step(inputs, targets)\n",
        "            preds_list.append(preds_one_batch.cpu().detach().numpy())\n",
        "            running_loss.update(loss.item(), data_loader.batch_size)\n",
        "            running_metrics.update(metrics, data_loader.batch_size)\n",
        "            tk0.set_postfix(valid_loss = running_loss.avg, validate_step_metrics = running_metrics.avg, stage=\"validation\")\n",
        "            wandb.log({\n",
        "                \"valid/step\" : batch_idx,\n",
        "                \"valid/metric_step\" : running_metrics.avg,\n",
        "                \"valid/loss\": running_loss.avg, \n",
        "                })\n",
        "        preds_arr = np.concatenate(preds_list)\n",
        "        valid_metric_val = self.epoch_metrics(preds_arr, self.valid_targets)\n",
        "        tk0.close()\n",
        "        return valid_metric_val, running_loss.avg\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        dataset,\n",
        "        batch_size = 16,\n",
        "        num_workers = 8,\n",
        "    ):\n",
        "        self.model.eval()\n",
        "        self.test_loader =  torch.utils.data.DataLoader(\n",
        "            dataset = test_dataset, \n",
        "            batch_size = batch_size,\n",
        "            shuffle = False, \n",
        "            num_workers= num_workers,\n",
        "            drop_last = False,\n",
        "            pin_memory = True\n",
        "        )\n",
        "        preds_list = []\n",
        "        tk0 = tqdm(data_loader, total=len(self.test_loader), position = 0, leave = True)\n",
        "        for batch_idx, (inputs, targets) in enumerate(tk0):\n",
        "            preds_one_batch = self.predict_one_step(inputs, targets)\n",
        "            preds_list.append(preds_one_batch.cpu().detach().numpy())\n",
        "            tk0.set_postfix(stage=\"inference\")\n",
        "        tk0.close()\n",
        "        preds_arr = np.concatenate(preds_list)\n",
        "        return preds_arr\n",
        "\n",
        "    def save(self, model_path):\n",
        "        model_state_dict = self.state_dict()\n",
        "        if self.optimizer is not None:\n",
        "            opt_state_dict = self.optimizer.state_dict()\n",
        "        else:\n",
        "            opt_state_dict = None\n",
        "        if self.scheduler_after_step is not None:\n",
        "            sch_state_dict_after_step = self.scheduler_after_step.state_dict()\n",
        "        else:\n",
        "            sch_state_dict_after_step = None\n",
        "        if self.scheduler_after_epoch is not None:\n",
        "            sch_state_dict_after_epoch = self.scheduler_after_epoch.state_dict()\n",
        "        else:\n",
        "            sch_state_dict_after_epoch = None\n",
        "        model_dict = {}\n",
        "        model_dict[\"state_dict\"] = model_state_dict\n",
        "        model_dict[\"optimizer\"] = opt_state_dict\n",
        "        model_dict[\"scheduler_after_step\"] = sch_state_dict_after_step\n",
        "        model_dict[\"scheduler_after_epoch\"] = sch_state_dict_after_epoch\n",
        "        model_dict[\"epoch\"] = self.current_epoch\n",
        "        model_dict[\"fp16\"] = self.fp16\n",
        "        model_dict[\"multiple_GPU\"] = self.multiple_GPU\n",
        "        torch.save(model_dict, model_path)\n",
        "\n",
        "    def load(self, model_path):\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        if next(self.parameters()).device != self.device:\n",
        "            self.to(self.device)\n",
        "        model_dict = torch.load(model_path, map_location=torch.device(device))\n",
        "        self.load_state_dict(model_dict[\"state_dict\"])\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        cfg,\n",
        "        checkpoint_save_path = '',\n",
        "        mode = 'max',\n",
        "        patience = 10,\n",
        "        delta = 0.001,\n",
        "    ):\n",
        "        set_seed(CFG.seed)\n",
        "        self._init_wandb(cfg)\n",
        "\n",
        "        if mode == 'max':\n",
        "            current_best_valid_metrics = -float('inf')\n",
        "        else:\n",
        "            current_best_valid_metrics= float('inf')\n",
        "        early_stopping_counter = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_loss = self.train_one_epoch(self.train_loader)\n",
        "            if valid_dataset:\n",
        "                valid_metrics, valid_loss = self.validate_one_epoch(self.valid_loader)\n",
        "                # Early Stopping and save at the check points.\n",
        "                if mode == 'max':\n",
        "                    if valid_metrics < current_best_valid_metrics + delta:\n",
        "                        early_stopping_counter += 1\n",
        "                        print(f'EarlyStopping counter: {early_stopping_counter} out of {patience}')\n",
        "                        if early_stopping_counter >= patience: break\n",
        "                    else:\n",
        "                        print(f\"Validation score improved ({current_best_valid_metrics} --> {valid_metrics}). Saving the check point!\")\n",
        "                        current_best_validmetrics= valid_metrics\n",
        "                        self.save(checkpoint_save_path + f\"{cfg.pretrained_model_name}_epoch{epoch}.cpt\" )\n",
        "                else:\n",
        "                    if valid_metrics > current_best_valid_metrics - delta:\n",
        "                        early_stopping_counter += 1\n",
        "                        print(f'EarlyStopping counter: {early_stopping_counter} out of {patience}')\n",
        "                        if early_stopping_counter >= patience: break\n",
        "                    else:\n",
        "                        print(f\"Validation score improved ({current_best_valid_metrics} --> {valid_metrics}). Saving the check point!\")\n",
        "                        current_best_valid_metrics = valid_metrics\n",
        "                        self.save(checkpoint_save_path + f\"{cfg.pretrained_model_name}_epoch{epoch}.cpt\" )\n",
        "                        \n",
        "            #writer.add_scalar(\"Loss/train\", 1.0, epoch)\n",
        "            print(f'epoch: {epoch}, validate_epoch_metrics : {valid_metrics}')\n",
        "            wandb.log({\n",
        "                \"epoch\" : epoch,\n",
        "                \"train/loss\" : train_loss,\n",
        "                \"valid/loss\" : valid_loss,\n",
        "                \"valid/metric\" : valid_metrics,\n",
        "                })\n",
        "        wandb.finish()\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547,
          "referenced_widgets": [
            "8d97cf14aba9409a817d480f1011743d",
            "73784d45422b44f5b529ec525a6bb37c",
            "e2f091c8ef3b45b6b9b401b86bc6d0db",
            "8aaeac2bc9244651a0b58e6b15e2496d",
            "59982be56d15496c987ef1d6ce6b66f9",
            "512b8abc8c7d4ed496eba9bccee3426a",
            "cc06a5d335bf43028258837bf6305750",
            "bc38748ce4da4cf2bb771eaaa51a86a9"
          ]
        },
        "id": "7mqPApu1TO-W",
        "outputId": "0b4cd26f-3724-4186-ac04-93ed161d547b"
      },
      "source": [
        "df3 = pd.read_csv('../input/seti-breakthrough-listen/valid_labels.csv')\n",
        "df3['img_path'] = df3['id'].apply(\n",
        "    lambda x: f'../input/seti-breakthrough-listen/valid/{x[0]}/{x}.npy'\n",
        ")\n",
        "\n",
        "X_valid = df3.img_path\n",
        "Y_valid = df3.target\n",
        "\n",
        "for fold_cnt, (train_index, test_index) in enumerate(skf.split(X, Y), 1):\n",
        "    train_images, valid_images = X[train_index], X_valid\n",
        "    train_targets, valid_targets = Y[train_index], Y_valid\n",
        "\n",
        "    train_dataset = ClassificationDataset(\n",
        "        image_paths=train_images, \n",
        "        targets=train_targets, \n",
        "        transform = train_aug\n",
        "    )\n",
        "    valid_dataset = ClassificationDataset(\n",
        "        image_paths=valid_images, \n",
        "        targets=valid_targets, \n",
        "        transform = None\n",
        "    )\n",
        "    model = BasicNN()\n",
        "\n",
        "    model.fit(\n",
        "        cfg = CFG,\n",
        "        train_dataset = train_dataset,\n",
        "        valid_dataset = valid_dataset,\n",
        "        valid_targets = valid_targets,\n",
        "        epochs = CFG.epochs,\n",
        "        train_batchsize = CFG.batch_size,\n",
        "        valid_batchsize = CFG.batch_size,\n",
        "        num_workers = CFG.num_workers,\n",
        "        fp16 = CFG.fp16,\n",
        "        checkpoint_save_path = CFG.checkpoint_path,\n",
        "        mode = CFG.patience_mode,\n",
        "        patience = CFG.patience,\n",
        "        delta = CFG.delta\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_v2s_ra2_288-a6477665.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2s_ra2_288-a6477665.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.32<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">efficientnetv2_rw_s_colab_tawara_bs16_adamW_default_lr1e-06_alpha0.5</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/yseeker/SETI_aug_check2\" target=\"_blank\">https://wandb.ai/yseeker/SETI_aug_check2</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/yseeker/SETI_aug_check2/runs/2mwdeaxw\" target=\"_blank\">https://wandb.ai/yseeker/SETI_aug_check2/runs/2mwdeaxw</a><br/>\n",
              "                Run data is saved locally in <code>/content/drive/My Drive/kaggle/working/wandb/run-20210626_160313-2mwdeaxw</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d97cf14aba9409a817d480f1011743d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2508.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-259a468ebcd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatience_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     )\n",
            "\u001b[0;32m<ipython-input-11-3e05d577a183>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, cfg, train_dataset, valid_dataset, valid_targets, epochs, train_batchsize, valid_batchsize, num_workers, determinstic, benchmark, fp16, multiple_GPU, checkpoint_save_path, mode, patience, delta)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0mvalid_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-3e05d577a183>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(self, data_loader)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mtk0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtk0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mpreds_one_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             \u001b[0mrunning_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mrunning_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-3e05d577a183>\u001b[0m in \u001b[0;36mtrain_one_step\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                     \u001b[0;31m#loss = self.criterion(outputs, targets.view(-1, 1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                     loss = mixup_criterion(self.criterion,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-3e05d577a183>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m#x = self.conv1(inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             for hook in itertools.chain(\n",
            "\u001b[0;32m/content/drive/My Drive/kaggle/input/pytorch-image-models-master/timm/models/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_rate\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/kaggle/input/pytorch-image-models-master/timm/models/efficientnet.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_stem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [24, 1, 3, 3], expected input[16, 256, 1638, 1] to have 1 channels, but got 256 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROU2w75yYfyX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLfmMG33HIH5"
      },
      "source": [
        "submission = pd.read_csv('../input/seti-breakthrough-listen/sample_submission.csv')\n",
        "submission['img_path'] = submission['id'].apply(\n",
        "    lambda x: f'../input/seti-breakthrough-listen/test/{x[0]}/{x}.npy'\n",
        ")\n",
        "test_dataset = ClassificationDataset(\n",
        "    image_paths=submission.img_path.values, \n",
        "    targets=submission.target.values, \n",
        "    transform = train_aug\n",
        ")\n",
        "\n",
        "final_preds = None\n",
        "num_of_ave = 5\n",
        "outs = []\n",
        "sig = torch.nn.Sigmoid()\n",
        "for model in models:\n",
        "    for j in range(num_of_ave):\n",
        "        preds = model.predict(test_dataset, batch_size=16, n_jobs=-1)\n",
        "        temp_preds = None\n",
        "        for p in preds:\n",
        "            if temp_preds is None:\n",
        "                temp_preds = p\n",
        "            else:\n",
        "                temp_preds = np.vstack((temp_preds, p))\n",
        "        if final_preds is None:\n",
        "            final_preds = temp_preds\n",
        "        else:\n",
        "            final_preds += temp_preds\n",
        "    final_preds = final_preds/num_of_ave\n",
        "    out = sig(torch.from_numpy(final_preds))\n",
        "    out = out.detach().numpy()\n",
        "    outs.append(out)\n",
        "pred = np.mean(np.array(outs), axis=0)\n",
        "submission.target = pred\n",
        "submission.drop(['img_path'], axis=1, inplace=True)\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLJJW08RHlRi"
      },
      "source": [
        "class ClassificationDataset():\n",
        "    def __init__(self, image_paths, targets, transform = None): \n",
        "        self.image_paths = image_paths\n",
        "        self.targets = targets\n",
        "        self.transform = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, item): \n",
        "        targets = self.targets[item]\n",
        "        #image1 = np.load(self.image_paths[item]).astype(float)\n",
        "        image1 = np.load(self.image_paths[item])[::2].astype(np.float32)\n",
        "        image = np.vstack(image1).transpose((1, 0))\n",
        "\n",
        "        image = ((image - np.mean(image, axis=1, keepdims=True)) / np.std(image, axis=1, keepdims=True))\n",
        "        image = ((image - np.mean(image, axis=0, keepdims=True)) / np.std(image, axis=0, keepdims=True))\n",
        "    \n",
        "        image = image.astype(np.float32)[np.newaxis, ]\n",
        "\n",
        "        # image = np.load(self.image_paths[item]).astype(np.float32)\n",
        "        # image = np.vstack(image).transpose((1, 0))\n",
        "        # image = cv2.resize(image, dsize=(224,224), interpolation=cv2.INTER_CUBIC)\n",
        "        # image = image[np.newaxis, :, :]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)[\"image\"]\n",
        "\n",
        "        return {\n",
        "            \"image\": torch.tensor(image, dtype=torch.float),\n",
        "            \"targets\": torch.tensor(targets, dtype=torch.float),\n",
        "        }\n",
        "\n",
        "\n",
        "class ClassificationDataset2():\n",
        "    def __init__(self, image_paths, targets, transform = None): \n",
        "        self.image_paths = image_paths\n",
        "        self.targets = targets\n",
        "        self.transform = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, item): \n",
        "        targets = self.targets[item]\n",
        "        #image1 = np.load(self.image_paths[item]).astype(float)\n",
        "        image1 = np.load(self.image_paths[item])[::2].astype(np.float32)\n",
        "        image = np.vstack(image1).transpose((1, 0))\n",
        "\n",
        "        image = ((image - np.mean(image, axis=1, keepdims=True)) / np.std(image, axis=1, keepdims=True))\n",
        "        image = ((image - np.mean(image, axis=0, keepdims=True)) / np.std(image, axis=0, keepdims=True))\n",
        "    \n",
        "        image = image.astype(np.float32)[np.newaxis, ]\n",
        "\n",
        "        # image = np.load(self.image_paths[item]).astype(np.float32)\n",
        "        # image = np.vstack(image).transpose((1, 0))\n",
        "        # image = cv2.resize(image, dsize=(224,224), interpolation=cv2.INTER_CUBIC)\n",
        "        # image = image[np.newaxis, :, :]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)[\"image\"]\n",
        "\n",
        "        return torch.tensor(image, dtype=torch.float), torch.tensor(targets, dtype=torch.float)\n",
        "\n",
        "df = pd.read_csv('../input/seti-breakthrough-listen/train_labels.csv')\n",
        "df['img_path'] = df['id'].apply(\n",
        "    lambda x: f'../input/seti-breakthrough-listen/train/{x[0]}/{x}.npy'\n",
        ")\n",
        "X = df.img_path.values\n",
        "Y = df.target.values\n",
        "\n",
        "\n",
        "\n",
        "skf = StratifiedKFold(n_splits = 5)\n",
        "\n",
        "patience = 3\n",
        "delta = 0.002\n",
        "\n",
        "for fold_cnt, (train_index, test_index) in enumerate(skf.split(X, Y), 1):\n",
        "    train_images, valid_images = X[train_index], X[test_index]\n",
        "    train_targets, valid_targets = Y[train_index], Y[test_index]\n",
        "\n",
        "\n",
        "\n",
        "    av = 0\n",
        "    for i in range(5):\n",
        "        train_dataset1 = ClassificationDataset(\n",
        "        image_paths=train_images, \n",
        "        targets=train_targets, \n",
        "        transform = None\n",
        "    )\n",
        "        train_loader1 = torch.utils.data.DataLoader(\n",
        "                        dataset = train_dataset1, \n",
        "                        batch_size = 32,\n",
        "                        shuffle=True, \n",
        "                        num_workers= 8,\n",
        "                        pin_memory = True\n",
        "                    )\n",
        "        d = datetime.datetime.now()\n",
        "        for i, data in enumerate(train_loader1):\n",
        "            if i == 0: d = datetime.datetime.now()\n",
        "            #print(i, datetime.datetime.now()-d)\n",
        "            if i == 100: break\n",
        "            pass\n",
        "        t = datetime.datetime.now() - d\n",
        "        #print(t)\n",
        "        av += t.seconds\n",
        "    print('tez-av: ', av/5, 'num_workers : ', 8)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY0Bk-inbtGC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}